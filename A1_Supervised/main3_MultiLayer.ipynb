{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Quick introduction to jupyter notebooks**\n",
    "* Each cell in this notebook contains either code or text.\n",
    "* You can run a cell by pressing Ctrl-Enter, or run and advance to the next cell with Shift-Enter.\n",
    "* Code cells will print their output, including images, below the cell. Running it again deletes the previous output, so be careful if you want to save some results.\n",
    "* You don't have to rerun all cells to test changes, just rerun the cell you have made changes to. Some exceptions might apply, for example if you overwrite variables from previous cells, but in general this will work.\n",
    "* If all else fails, use the \"Kernel\" menu and select \"Restart Kernel and Clear All Output\". You can also use this menu to run all cells.\n",
    "* A useful debug tool is the console. You can right-click anywhere in the notebook and select \"New console for notebook\". This opens a python console which shares the environment with the notebook, which let's you easily print variables or test commands.\n",
    "\n",
    "### **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorst\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Automatically reload modules when changed\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# Plot figures \"inline\" with other output\n",
    "%matplotlib inline\n",
    "\n",
    "# Import modules, classes, functions\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils import loadDataset, splitData, plotProgress, plotProgressNetworkMulti, \\\n",
    "    plotResultsDots, plotIsolines, plotConfusionMatrixOCR, plotResultsDotsGradient\n",
    "from evalFunctions import calcAccuracy, calcConfusionMatrix\n",
    "\n",
    "# Configure nice figures\n",
    "plt.rcParams['figure.facecolor']='white'\n",
    "plt.rcParams['figure.figsize']=(8, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***! IMPORTANT NOTE !***\n",
    "\n",
    "Your implementation should only use the `numpy` (`np`) module. The `numpy` module provides all the functionality you need for this assignment and makes it easier debuging your code. No other modules, e.g. `scikit-learn` or `scipy` among others, are allowed and solutions using modules other than `numpy` will be sent for re-submission. You can find everything you need about `numpy` in the official [documentation](https://numpy.org/doc/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Multi-layer neural network**\n",
    "\n",
    "The implementation of the multi-layer network is a bit beyond the scope of this course, so we will provide it for you. Your task is instead to optimize the training and interpret tthe results.\n",
    "\n",
    "Similar to the single-layer network, the multi-layer keep track of weight matrices `W1` and `W2`, and bias vectors `B1` and `B2`. The structure of the code is still the same, with a `forward`, `backward`, and `update` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.1 Implementing the forward pass**\n",
    "\n",
    "This implementation will look similar to the single-layer code, but note that it also returns the intemediate variable `U`, which is the output of the hidden layer after passing throught the activation function. This will be used in the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W1, B1, W2, B2, useTanhOutput=False):\n",
    "    \"\"\"Forward pass of two layer network\n",
    "\n",
    "    Args:\n",
    "        X (array): Input samples.\n",
    "        W1 (array): First layer neural network weights.\n",
    "        B1 (array): First layer neural network biases.\n",
    "        W2 (array): Second layer neural network weights.\n",
    "        B2 (array): Second layer neural network biases.\n",
    "\n",
    "    Returns:\n",
    "        Y (array): Output for each sample and class.\n",
    "        L (array): Resulting label of each sample.\n",
    "        U (array): Output of hidden layer.\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # === Your code here ========================= \n",
    "    # --------------------------------------------\n",
    "    \n",
    "    U = np.tanh(X @ W1 + B1) if useTanhOutput else X @ W1 + B1\n",
    "    \n",
    "    Y = np.tanh(U @ W2 + B2) if useTanhOutput else U @ W2 + B2\n",
    "    \n",
    "    # ============================================\n",
    "    \n",
    "    # Calculate labels\n",
    "    L = Y.argmax(axis=1)\n",
    "\n",
    "    return Y, L, U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.2 Implementing the backward pass**\n",
    "\n",
    "This backward pass implementation uses the so called delta notation. This is very useful when implementing the general case of an N-layer network, since it can easily be implemnted in an iterative manner in a loop going over each layer. We will not do that in this assignment, but it is good to know that this is essentially how the popular frameworks for deep learning functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(W1, B1, W2, B2, X, U, Y, D, useTanhOutput=False):\n",
    "    \"\"\"Compute the gradients for network weights and biases\n",
    "\n",
    "    Args:\n",
    "        W1 (array): Current values of the layer 1 network weights.\n",
    "        B1 (array): Current values of the layer 1 network biases.\n",
    "        W2 (array): Current values of the layer 2 network weights.\n",
    "        B2 (array): Current values of the layer 2 network biases.\n",
    "        X (array): Training samples.\n",
    "        U (array): Intermediate outputs of the hidden layer.\n",
    "        Y (array): Predicted outputs.\n",
    "        D (array): Target outputs.\n",
    "        \n",
    "        useTanhOutput (bool): (optional)\n",
    "            True  - Network uses tanh activation on output layer\n",
    "            False - Network uses linear (no) activation on output layer\n",
    "        \n",
    "    Returns:\n",
    "        GradW1 (array): Gradients with respect to W1\n",
    "        GradB1 (array): Gradients with respect to B1\n",
    "        GradW2 (array): Gradients with respect to W2\n",
    "        GradB2 (array): Gradients with respect to B2\n",
    "    \"\"\"\n",
    "    \n",
    "    N  = Y.shape[0]\n",
    "    NC = Y.shape[1]\n",
    "    \n",
    "    # --------------------------------------------\n",
    "    # === Your code here =========================\n",
    "    # --------------------------------------------\n",
    "    \n",
    "    # Gradient for the output layer\n",
    "    GradW2 = 2 * U.T @ (Y - D) * ((1 - Y**2) if useTanhOutput else 1)\n",
    "    GradB2 = 2 * (Y - D) * ((1 - Y**2) if useTanhOutput else 1)\n",
    "\n",
    "    # And the input layer\n",
    "    GradW1 = 2 * X.T @ (((Y - D) @ W1.T) * (1 - U**2))\n",
    "    GradB1 = 2 * (Y - D) * ((1 - Y**2) if useTanhOutput else 1) @ W1.T * (1 - U**2)\n",
    "    \n",
    "    # ============================================\n",
    "    \n",
    "    return GradW1, GradB1, GradW2, GradB2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.3 Implementing the weight update**\n",
    "\n",
    "For this update function we have implemented a more powerful algorithm called momentum gradient descent. This is part of an optional task at the end of the notebook, and you do not have to use it unless you want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(W1, B1, W2, B2, GradW1, GradB1, GradW2, GradB2, params):\n",
    "    \"\"\"Update weights and biases using computed gradients.\n",
    "\n",
    "    Args:\n",
    "        W1 (array): Current values of the layer 1 network weights.\n",
    "        B1 (array): Current values of the layer 1 network biases.\n",
    "        W2 (array): Current values of the layer 2 network weights.\n",
    "        B2 (array): Current values of the layer 2 network biases.\n",
    "        \n",
    "        GradW1 (array): Gradients with respect to W1.\n",
    "        GradB1 (array): Gradients with respect to B1.\n",
    "        GradW2 (array): Gradients with respect to W2.\n",
    "        GradB2 (array): Gradients with respect to B2.\n",
    "        \n",
    "        params (dict):\n",
    "            - learningRate: Scale factor for update step.\n",
    "            - momentum: Scale factor for momentum update (optional).\n",
    "        \n",
    "    Returns:\n",
    "        W1 (array): Updated layer 1 weights.\n",
    "        B1 (array): Updated layer 1 biases.\n",
    "        W2 (array): Updated layer 2 weights.\n",
    "        B2 (array): Updated layer 2 biases.\n",
    "    \"\"\"\n",
    "    \n",
    "    LR = params[\"learningRate\"]\n",
    "    \n",
    "    # Uncomment this is you are working on the optional task on momentum.\n",
    "    # M = params[\"momentum\"]\n",
    "    # PrevGradW1 = params[\"PrevGradW1\"]\n",
    "    # PrevGradB1 = params[\"PrevGradB1\"]\n",
    "    # PrevGradW2 = params[\"PrevGradW2\"]\n",
    "    # PrevGradB2 = params[\"PrevGradB2\"]\n",
    "    \n",
    "    # --------------------------------------------\n",
    "    # === Your code here =========================\n",
    "    # --------------------------------------------\n",
    "    \n",
    "    # Update weights\n",
    "    W1 = W1 - LR * GradW1\n",
    "    B1 = B1 - LR * GradB1\n",
    "    W2 = W2 - LR * GradW2\n",
    "    B2 = B2 - LR * GradB2\n",
    "    \n",
    "    # Uncomment this is you are working on the optional task on momentum.\n",
    "    # params[\"PrevGradW1\"] = ???\n",
    "    # params[\"PrevGradB1\"] = ???\n",
    "    # params[\"PrevGradW2\"] = ???\n",
    "    # params[\"PrevGradB2\"] = ???\n",
    "    \n",
    "    # ============================================\n",
    "    \n",
    "    return W1, B1, W2, B2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.4 The training function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainMultiLayer(XTrain, DTrain, XTest, DTest, W1_0, B1_0, W2_0, B2_0, params):\n",
    "    \"\"\"Trains a two-layer network\n",
    "\n",
    "    Args:\n",
    "        XTrain (array): Training samples.\n",
    "        DTrain (array): Training network target values.\n",
    "        XTest (array): Test samples.\n",
    "        DTest (array): Test network target values.\n",
    "        W1_0 (array): Initial values of the first layer network weights.\n",
    "        B1_0 (array): Initial values of the first layer network biases.\n",
    "        W2_0 (array): Initial values of the second layer network weights.\n",
    "        B2_0 (array): Initial values of the second layer network biases.\n",
    "        params (dict): Dictionary containing:\n",
    "            epochs (int): Number of training steps.\n",
    "            learningRate (float): Size of a training step.\n",
    "\n",
    "    Returns:\n",
    "        W1 (array): First layer weights after training.\n",
    "        B1 (array): Fisrt layer biases after training.\n",
    "        W2 (array): Second layer weights after training.\n",
    "        B2 (array): Second layer biases after training.\n",
    "        metrics (dict): Losses and accuracies for training and test data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize variables\n",
    "    metrics = {keys:np.zeros(params[\"epochs\"]+1) for keys in [\"lossTrain\", \"lossTest\", \"accTrain\", \"accTest\"]}\n",
    "\n",
    "    if \"useTanhOutput\" not in params:\n",
    "        params[\"useTanhOutput\"] = False\n",
    "        \n",
    "    if \"momentum\" not in params:\n",
    "        params[\"momentum\"] = 0       \n",
    "    \n",
    "    nTrain = XTrain.shape[0]\n",
    "    nTest  = XTest.shape[0]\n",
    "    nClasses = DTrain.shape[1]\n",
    "    \n",
    "    # Set initial weights\n",
    "    W1 = W1_0\n",
    "    B1 = B1_0\n",
    "    W2 = W2_0\n",
    "    B2 = B2_0\n",
    "    \n",
    "    # For optional task on momentum\n",
    "    params[\"PrevGradW1\"] = np.zeros_like(W1)\n",
    "    params[\"PrevGradB1\"] = np.zeros_like(B1)\n",
    "    params[\"PrevGradW2\"] = np.zeros_like(W2)\n",
    "    params[\"PrevGradB2\"] = np.zeros_like(B2)\n",
    "\n",
    "    # Get class labels\n",
    "    LTrain = np.argmax(DTrain, axis=1)\n",
    "    LTest  = np.argmax(DTest , axis=1)\n",
    "\n",
    "    # Calculate initial metrics\n",
    "    YTrain, LTrainPred, UTrain = forward(XTrain, W1, B1, W2, B2, params[\"useTanhOutput\"])\n",
    "    YTest , LTestPred , _      = forward(XTest , W1, B1, W2, B2, params[\"useTanhOutput\"])\n",
    "    \n",
    "    # Including the initial metrics makes the progress plots worse, set nan to exclude\n",
    "    metrics[\"lossTrain\"][0] = np.nan # ((YTrain - DTrain)**2).mean()\n",
    "    metrics[\"lossTest\"][0]  = np.nan # ((YTest  - DTest )**2).mean()\n",
    "    metrics[\"accTrain\"][0]  = np.nan # (LTrainPred == LTrain).mean()\n",
    "    metrics[\"accTest\"][0]   = np.nan # (LTestPred  == LTest ).mean()\n",
    "\n",
    "    # Create figure for plotting progress\n",
    "    fig = plt.figure(figsize=(20,8), tight_layout=True)\n",
    "\n",
    "    # Training loop\n",
    "    for n in range(1, params[\"epochs\"]+1):\n",
    "        \n",
    "        # --------------------------------------------\n",
    "        # === This is the important part =============\n",
    "        # === where your code is applied =============\n",
    "        # --------------------------------------------\n",
    "        \n",
    "        # Compute gradients...\n",
    "        GradW1, GradB1, GradW2, GradB2 = backward(W1, B1, W2, B2, XTrain, UTrain, YTrain, DTrain, params[\"useTanhOutput\"])\n",
    "        # ... and update weights\n",
    "        W1, B1, W2, B2 = update(W1, B1, W2, B2, GradW1, GradB1, GradW2, GradB2, params)\n",
    "        \n",
    "        # ============================================\n",
    "        \n",
    "        # Evaluate errors\n",
    "        YTrain, LTrainPred, UTrain = forward(XTrain, W1, B1, W2, B2, params[\"useTanhOutput\"])\n",
    "        YTest , LTestPred , _      = forward(XTest , W1, B1, W2, B2, params[\"useTanhOutput\"])\n",
    "        metrics[\"lossTrain\"][n] = ((YTrain - DTrain)**2).mean()\n",
    "        metrics[\"lossTest\"][n]  = ((YTest  - DTest )**2).mean()\n",
    "        metrics[\"accTrain\"][n]  = (LTrainPred == LTrain).mean()\n",
    "        metrics[\"accTest\"][n]   = (LTestPred  == LTest ).mean()\n",
    "\n",
    "        # Plot progress\n",
    "        if (plotProgress and not n % 500) or n == params[\"epochs\"]:\n",
    "            if W1.shape[0] == 2 and W1.shape[1] <= 8:\n",
    "                plotProgressNetworkMulti(fig, W1, B1, W2, B2, metrics, n=n)\n",
    "            else:\n",
    "                plotProgress(fig, metrics, n)\n",
    "\n",
    "    return W1, B1, W2, B2, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define the same function for normalizing the data, which is even more important now that we have more than one layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    # Compute mean and std\n",
    "    m = X.mean(axis=0)\n",
    "    s = X.std(axis=0)\n",
    "    # Prevent division by 0 is feature has no variance\n",
    "    s[s == 0] = 1\n",
    "    # Return normalized data\n",
    "    return (X - m) / s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 1:</span>**\n",
    "\n",
    "Explain why large, non-normalized input features might be a problem when using two layers, but not when using a single layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\n",
    "\\[ Your answers here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **2 Optimizing each dataset**\n",
    "\n",
    "Like before, we define a function that performs the boilerplate code for training the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  trainMultiLayerOnDataset(datasetNr, testSplit, W1_0, B1_0, W2_0, B2_0, params):\n",
    "    \"\"\"Train a two layer network on a specific dataset.\n",
    "\n",
    "    Ags:\n",
    "        datasetNr (int): ID of dataset to use\n",
    "        testSplit (float): Fraction of data reserved for testing.\n",
    "        W1_0 (array): Initial values of the first layer network weights.\n",
    "        B1_0 (array): Initial values of the first layer network biases.\n",
    "        W2_0 (array): Initial values of the second layer network weights.\n",
    "        B2_0 (array): Initial values of the second layer network biases.\n",
    "        params (dict): Dictionary containing:\n",
    "            nIterations (int): Number of training steps.\n",
    "            learningRate (float): Size of a training step.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data and split into training and test sets\n",
    "    X, D, L = loadDataset(datasetNr)\n",
    "    XTrain, DTrain, LTrain, XTest, DTest, LTest = splitData(X, D, L, testSplit)\n",
    "    \n",
    "    if \"normalize\" in params and params[\"normalize\"]:\n",
    "        XTrainNorm = normalize(XTrain)\n",
    "        XTestNorm  = normalize(XTest)\n",
    "    else:\n",
    "        XTrainNorm = XTrain\n",
    "        XTestNorm  = XTest\n",
    "\n",
    "    # Train network\n",
    "    W1, B1, W2, B2, metrics = trainMultiLayer(XTrainNorm, DTrain, XTestNorm, DTest, W1_0, B1_0, W2_0, B2_0, params)\n",
    "\n",
    "    # Predict classes on test set\n",
    "    LPredTrain = forward(XTrainNorm, W1, B1, W2, B2, params[\"useTanhOutput\"])[1]\n",
    "    LPredTest  = forward(XTestNorm , W1, B1, W2, B2, params[\"useTanhOutput\"])[1]\n",
    "\n",
    "    # Compute metrics\n",
    "    accTrain = calcAccuracy(LPredTrain, LTrain)\n",
    "    accTest  = calcAccuracy(LPredTest , LTest)\n",
    "    confMatrix = calcConfusionMatrix(LPredTest, LTest)\n",
    "\n",
    "    # Display results\n",
    "    print(f'Train accuracy: {accTrain:.4f}')\n",
    "    print(f'Test accuracy: {accTest:.4f}')\n",
    "    print(\"Test data confusion matrix:\")\n",
    "    print(confMatrix)\n",
    "\n",
    "    if datasetNr < 4:\n",
    "        # Switch between these two functions to see another way to visualize the network output.\n",
    "        plotResultsDots(XTrainNorm, LTrain, LPredTrain, XTestNorm, LTest, LPredTest, lambda X: forward(X, W1, B1, W2, B2, params[\"useTanhOutput\"])[1])\n",
    "        #plotResultsDotsGradient(XTrainNorm, LTrain, LPredTrain, XTestNorm, LTest, LPredTest, lambda X: forward(X, W1, B1, W2, B2, params[\"useTanhOutput\"])[0])\n",
    "    else:\n",
    "        plotConfusionMatrixOCR(XTest, LTest, LPredTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.1 Optimizing dataset 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 10 is different from 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m10\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearningRate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalize\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124museTanhOutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ============================================\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mtrainMultiLayerOnDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB1_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB2_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 28\u001b[0m, in \u001b[0;36mtrainMultiLayerOnDataset\u001b[1;34m(datasetNr, testSplit, W1_0, B1_0, W2_0, B2_0, params)\u001b[0m\n\u001b[0;32m     25\u001b[0m     XTestNorm  \u001b[38;5;241m=\u001b[39m XTest\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Train network\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m W1, B1, W2, B2, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrainMultiLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXTrainNorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDTrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXTestNorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDTest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB1_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB2_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Predict classes on test set\u001b[39;00m\n\u001b[0;32m     31\u001b[0m LPredTrain \u001b[38;5;241m=\u001b[39m forward(XTrainNorm, W1, B1, W2, B2, params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124museTanhOutput\u001b[39m\u001b[38;5;124m\"\u001b[39m])[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[1;32mIn[5], line 76\u001b[0m, in \u001b[0;36mtrainMultiLayer\u001b[1;34m(XTrain, DTrain, XTest, DTest, W1_0, B1_0, W2_0, B2_0, params)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     69\u001b[0m     \n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# --------------------------------------------\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \n\u001b[0;32m     75\u001b[0m     \u001b[38;5;66;03m# Compute gradients...\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     GradW1, GradB1, GradW2, GradB2 \u001b[38;5;241m=\u001b[39m \u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXTrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mUTrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYTrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDTrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43museTanhOutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m# ... and update weights\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     W1, B1, W2, B2 \u001b[38;5;241m=\u001b[39m update(W1, B1, W2, B2, GradW1, GradB1, GradW2, GradB2, params)\n",
      "Cell \u001b[1;32mIn[3], line 37\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(W1, B1, W2, B2, X, U, Y, D, useTanhOutput)\u001b[0m\n\u001b[0;32m     34\u001b[0m GradB2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (Y \u001b[38;5;241m-\u001b[39m D) \u001b[38;5;241m*\u001b[39m ((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m Y\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m useTanhOutput \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# And the input layer\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m GradW1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m X\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m ((\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m U\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m     38\u001b[0m GradB1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (Y \u001b[38;5;241m-\u001b[39m D) \u001b[38;5;241m*\u001b[39m ((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m Y\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m useTanhOutput \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m@\u001b[39m W1\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m U\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# ============================================\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 10 is different from 2)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here ========================= \n",
    "# --------------------------------------------\n",
    "nInputs  = 2\n",
    "nClasses = 2\n",
    "nHidden  = 10\n",
    "\n",
    "W1_0 = np.full((nInputs, nHidden), 123)\n",
    "B1_0 = np.full((1, nHidden), 123)\n",
    "W2_0 = np.full((nHidden, nClasses), 123)\n",
    "B2_0 = np.full((1, nClasses), 123)\n",
    "\n",
    "params = {\"epochs\": 10, \"learningRate\": 0.01, \"normalize\": False, \"useTanhOutput\": False}\n",
    "# ============================================\n",
    "\n",
    "trainMultiLayerOnDataset(1, 0.15, W1_0, B1_0, W2_0, B2_0, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 2:</span>**\n",
    "\n",
    "Optimize the training until you reach at least 98% test accuracy. Briefly motivate your choice of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\n",
    "\\[ Your answers here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.2 Optimizing dataset 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here ========================= \n",
    "# --------------------------------------------\n",
    "nInputs  = ???\n",
    "nClasses = ???\n",
    "nHidden  = ???\n",
    "\n",
    "W1_0 = ???\n",
    "B1_0 = ???\n",
    "W2_0 = ???\n",
    "B2_0 = ???\n",
    "\n",
    "params = {\"epochs\": ???, \"learningRate\": ???, \"normalize\": False, \"useTanhOutput\": False}\n",
    "# ============================================\n",
    "\n",
    "trainMultiLayerOnDataset(2, 0.15, W1_0, B1_0, W2_0, B2_0, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 3:</span>**\n",
    "\n",
    "Optimize the training until you reach at least 99% test accuracy. Briefly motivate your choice of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\n",
    "\\[ Your answers here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.3 Optimizing dataset 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here ========================= \n",
    "# --------------------------------------------\n",
    "nInputs  = ???\n",
    "nClasses = ???\n",
    "nHidden  = ???\n",
    "\n",
    "W1_0 = ???\n",
    "B1_0 = ???\n",
    "W2_0 = ???\n",
    "B2_0 = ???\n",
    "\n",
    "params = {\"epochs\": ???, \"learningRate\": ???, \"normalize\": False, \"useTanhOutput\": False}\n",
    "# ============================================\n",
    "\n",
    "trainMultiLayerOnDataset(3, 0.15, W1_0, B1_0, W2_0, B2_0, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 4:</span>**\n",
    "\n",
    "Optimize the training until you reach at least 99% test accuracy. Briefly motivate your choice of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\n",
    "\\[ Your answers here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.4 Optimizing dataset 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here ========================= \n",
    "# --------------------------------------------\n",
    "nInputs  = ???\n",
    "nClasses = ???\n",
    "nHidden  = ???\n",
    "\n",
    "W1_0 = ???\n",
    "B1_0 = ???\n",
    "W2_0 = ???\n",
    "B2_0 = ???\n",
    "\n",
    "params = {\"epochs\": ???, \"learningRate\": ???, \"normalize\": False, \"useTanhOutput\": False}\n",
    "# ============================================\n",
    "\n",
    "trainMultiLayerOnDataset(4, 0.15, W1_0, B1_0, W2_0, B2_0, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 5:</span>**\n",
    "\n",
    "Optimize the training until you reach at least 96% test accuracy. Briefly motivate your choice of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\n",
    "\\[ Your answers here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **3. Optional tasks**\n",
    "\n",
    "Here are some optional tasks that you can try if you are interested to learn more.\n",
    "\n",
    "#### **3.1 Tanh output activations**\n",
    "Enable *tanh* activation in the output layer and re-train the networks in section 2. Do you see any differences in the convergence speed or the decision boundaries? It will probably be more interesting to use the `plotResultsDotsGradient` function for this experiment.\n",
    "\n",
    "#### **3.2 Momentum gradient descent**\n",
    "So far, you have used normal gradient descent without any modifications. This update rule often suffers from a slow convergence speed, and often gets stuck in local minima. There are many more advanced weight update algorithms that try to fix this problem. The most simple upgrade is to use a momentum term that remembers the previous gradients and uses both those and the new to update the weights. In particular, normal gradient descent is defined as\n",
    "\n",
    "$$ \\large W \\leftarrow W - \\alpha \\nabla W $$\n",
    "\n",
    "with learning rate $\\alpha$. Momentum gradient descent is instead defined as\n",
    "\n",
    "$$ \\large G_t = \\beta G_{t-1} + \\alpha \\nabla W $$\n",
    "\n",
    "$$ \\large W \\leftarrow W - G_t $$\n",
    "\n",
    "where $\\beta$ is called the momentum term. A typical value of $\\beta$ is 0.9, which means previous gradients decays exponetially with exponent 0.9. Intuitively, momentum can be though of in the physical sense, where a ball rolling down a hill retains momentum and therefore can get over small bumps in the hill, even if they temporarilly go upwards. The decay of old gradient can then be thought of as friction. Normal gradient descent has none of these physical intuitions, and will immediately get stuck if it encounters a local minima.\n",
    "\n",
    "Your task is to rerun the training when using the momentum parameter. A good showcase of momentum should be dataset 4, which should converge much faster when using $\\beta = 0.9$ compared to $\\beta = 0$. If you want to go even further after this, you can take a look at [this](https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6) blog post which neatly summarizes some of the most common update rules even more powerful than momentum.\n",
    "\n",
    "#### **3.3 Hyperparameter search**\n",
    "\n",
    "Manually optimizing the hyperparameters can get tedious, so why not do it automatically? If you have no problems with your computer becomming a heat radiator, an interesting experiment is to implement a grid search over some hyperparameters, for example learning rate, momentum, and size of the hidden layer, and see where the optimal combination is. You probably have to modify the `trainMultiLayerOnDataset` function in two ways to do this.\n",
    "1. Return the metric you want to optimize, for example the final test loss.\n",
    "2. Provide a `seed` input to the `splitData` function, to make sure the grid search always uses the same data for each combination of hyper parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
